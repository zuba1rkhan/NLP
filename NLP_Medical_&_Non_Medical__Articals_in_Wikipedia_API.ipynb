{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGMnFn6ffymeRYbOx6O9iJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuba1rkhan/NLP/blob/main/NLP_Medical_%26_Non_Medical__Articals_in_Wikipedia_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvxRCNa12Ssn",
        "outputId": "1b4240b4-0960-4e1a-f90c-618a648a7a9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=83371e66d1152419f9ae48471c97c714ef2742254078b5885e0318677cfd89af\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YbCooMAj0k0D"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer as stemmer\n",
        "from nltk import FreqDist\n",
        "from nltk.classify import apply_features\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_sp_char(text):\n",
        "  remove_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "  return remove_text"
      ],
      "metadata": {
        "id": "fRSyPv1l1v4n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_low_case(text):\n",
        "  low_case_text = text.lower()\n",
        "  return low_case_text"
      ],
      "metadata": {
        "id": "m-E1mrq54tny"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_tokenize_sp_case(text):\n",
        "  tokens_word = word_tokenize(text)\n",
        "  stem_text = ' '.join([stemmer.stem(word)for word in tokens_word])\n",
        "  return stem_text\n"
      ],
      "metadata": {
        "id": "uVoWlw075MmF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "  token_word = word_tokenize(text)\n",
        "  stop_text = set(stopwords.words('english'))\n",
        "  cleaned_text = ' '.join([word for word in token_word if word.lower() not in stop_text])\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "hOlud7IdCDkc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_text(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    cleaned_text = soup.get_text()\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "SfL7w17ZFUW9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  token_text = nltk.word_tokenize(text)\n",
        "  lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in token_text)\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "VNqJKDw1GyQR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text):\n",
        "  stemmed = SnowballStemmer('english')\n",
        "  token_text = nltk.word_tokenize(text)\n",
        "  stemmed_text = ' '.join(stemmed.stem(word) for word in token_text)\n",
        "  return stemmed_text"
      ],
      "metadata": {
        "id": "MZHXMtXjtYrX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  filtered_text = remove_sp_char(text)\n",
        "  filtered_text = convert_low_case(text)\n",
        "  #filtered_text = stem_tokenize_sp_case(text)\n",
        "  filtered_text = lemmatize_text(text)\n",
        "  filtered_text = stem_text(text)\n",
        "  filtered_text = remove_stop_words(text)\n",
        "  filtered_text = remove_html_text(text)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "kwyyGSsjFzaw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_articles_by_category(category, language = 'en', num_results=10000):\n",
        "  base_url = 'https://en.wikipedia.org/w/api.php'.format(language)\n",
        "  parameters = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'list': 'categorymembers',\n",
        "        'cmtitle': 'Category:' + category,\n",
        "        'cmlimit': num_results\n",
        "  }\n",
        "  response = requests.get(base_url, parameters)\n",
        "  data = response.json()\n",
        "\n",
        "  if 'query' in data and 'categorymembers' in data['query']:\n",
        "    articles = [entry['title'] for entry in data['query']['categorymembers']]\n",
        "    return articles\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "b0nkwwUZUpDC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical = ['medicine', 'medicine', 'doctor', 'nurse']\n",
        "non_medical = ['history', 'Engineering', 'computer science', 'geography']\n",
        "medical_articles = []\n",
        "non_medical_articles = []\n",
        "for i in range(len(medical)):\n",
        "  medical_category = get_wikipedia_articles_by_category(medical[i],)\n",
        "  medical_articles.append(medical_category)\n",
        "medical_articles = [article for articles in medical_articles for article in articles]\n",
        "print(\"Medical articles: \", medical_articles)\n",
        "for j in range(len(non_medical)):\n",
        "  non_medical_category = get_wikipedia_articles_by_category(non_medical[j],)\n",
        "  non_medical_articles.append(non_medical_category)\n",
        "\n",
        "non_medical_articles = [article for articles in non_medical_articles for article in articles]\n",
        "\n",
        "print('\\nNon Medical Articles: ', non_medical_articles)\n",
        "\n",
        "labeled_data = [\n",
        "    {'text': article, 'label': 1} for article in medical_articles\n",
        "] + [\n",
        "    {'text': article, 'label': 0} for article in non_medical_articles\n",
        "]\n",
        "import pandas as pd\n",
        "article_df = pd.DataFrame(labeled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPwXXVYJX7ZR",
        "outputId": "19237b19-f71a-4dd6-929b-c48fcd878b57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical articles:  ['Medicine', 'Outline of medicine', 'Portal:Medicine', 'Category:Medicine by city', 'Category:Medicine by country', 'Category:Medicine by country subdivision', 'Category:Medicine by dependent territory', 'Category:Medicine by century', 'Category:Medicine by decade', 'Category:Medicine by year', 'Category:Medical lists', 'Category:Alternative medicine', 'Category:Veterinary medicine', 'Category:Clinical medicine', 'Category:Medical activism', 'Category:Medical associations', 'Category:Cause (medicine)', 'Category:Chemicals in medicine', 'Category:Medical-related conspiracy theories', 'Category:Medical culture', 'Category:Medicine deities', 'Category:Medical education', 'Category:History of medicine', 'Category:Health insurance', 'Category:Intersex and medicine', 'Category:Intersex variations', 'Category:Medical aspects of death', 'Category:Medical diplomacy', 'Category:Medical families', 'Category:Medical humanities', 'Category:Medical monitoring', 'Category:Medical phenomena', 'Category:Medical prevention', 'Category:Medical texts', 'Category:Medical volunteerism', 'Category:Medical and health organizations', 'Category:Practice of medicine', 'Category:Religion and medicine', 'Category:Medical research', 'Category:Medicine in society', 'Category:Medical symbols', 'Category:Medical technology', 'Category:Medical terminology', 'Category:Theory of medicine', 'Category:Works about medicine', 'Category:Medicine stubs', 'Medicine', 'Outline of medicine', 'Portal:Medicine', 'Category:Medicine by city', 'Category:Medicine by country', 'Category:Medicine by country subdivision', 'Category:Medicine by dependent territory', 'Category:Medicine by century', 'Category:Medicine by decade', 'Category:Medicine by year', 'Category:Medical lists', 'Category:Alternative medicine', 'Category:Veterinary medicine', 'Category:Clinical medicine', 'Category:Medical activism', 'Category:Medical associations', 'Category:Cause (medicine)', 'Category:Chemicals in medicine', 'Category:Medical-related conspiracy theories', 'Category:Medical culture', 'Category:Medicine deities', 'Category:Medical education', 'Category:History of medicine', 'Category:Health insurance', 'Category:Intersex and medicine', 'Category:Intersex variations', 'Category:Medical aspects of death', 'Category:Medical diplomacy', 'Category:Medical families', 'Category:Medical humanities', 'Category:Medical monitoring', 'Category:Medical phenomena', 'Category:Medical prevention', 'Category:Medical texts', 'Category:Medical volunteerism', 'Category:Medical and health organizations', 'Category:Practice of medicine', 'Category:Religion and medicine', 'Category:Medical research', 'Category:Medicine in society', 'Category:Medical symbols', 'Category:Medical technology', 'Category:Medical terminology', 'Category:Theory of medicine', 'Category:Works about medicine', 'Category:Medicine stubs']\n",
            "\n",
            "Non Medical Articles:  ['History', 'Glossary of history', 'Outline of history', 'Bajrangarh Fort', 'Biography', 'List of historical classifications', 'Economy of the Qing dynasty', 'Environmental history', 'Historical figure', 'Hijackers in the September 11 attacks', 'Historical culture', 'Historical significance', 'Historicism', 'History of catering', 'History of ice hockey', 'History of magic', 'History of science', 'Local history', 'Mancos Opera House', 'National memory', 'Oral history', 'Redemptive violence', 'Social history', 'Translation', 'History of Western civilization', 'Category:History by ethnic group', 'Category:History by location', 'Category:History by mountain range', 'Category:History by period', 'Category:Fields of history', 'Category:Historiography', 'Category:People in history occupations', 'Category:Chronology', 'Category:Origins', 'Category:Outlines of history and events', 'Category:History-related lists', 'Category:History awards', 'Category:Historical controversies', 'Category:History in culture', 'Category:History education', 'Category:Historical geography', 'Category:Historicity', 'Category:Legacies', 'Category:Historical objects', 'Category:History organizations', 'Category:Philosophy of history', 'Category:Historic preservation', 'Category:Pseudohistory', 'Category:Historical works', 'Category:History images', 'Category:History stubs', 'Engineering', 'Portal:Engineering', 'Outline of engineering', 'Aerospike engine', 'Asset management', 'Bug (engineering)', 'Charenton Metro-Viaduct', 'European Engineer', 'Generalized renewal process', 'Glossary of engineering: A–L', 'Glossary of engineering: M–Z', 'Glossary of microelectronics manufacturing terms', 'Integrity engineering', 'Marine construction', 'Metal assisted chemical etching', 'Oilfield scale inhibition', 'PDF/E', 'SLR Consulting', 'Stanford University School of Engineering', 'Stepped profile', 'Superconducting nanowire single-photon detector', 'Third medium contact method', 'Ezio Todini', 'UNESCO World Engineering Day for Sustainable Development', 'Lists of unsolved problems', 'Category:Engineering disciplines', 'Category:Engineers', 'Category:Women in engineering', 'Category:Engineering-related lists', 'Category:Engineering awards', 'Category:Engineering competitions', 'Category:Engineering concepts', 'Category:Engineering education', 'Category:Engineering organizations in Nepal', 'Category:Engineering software', 'Category:Engineering equipment', 'Category:History of engineering', 'Category:Industrial equipment', 'Category:Engineering literature', 'Category:Manufacturing', 'Category:Engineering occupations', 'Category:Engineering organizations', 'Category:Engineering projects', 'Category:Engineering studies', 'Category:Technology', 'Category:Engineering images', 'Category:Engineering stubs', 'Computer science', 'Glossary of computer science', 'Agnostic (data)', 'Boolean', 'Compressed cover tree', 'Computational social choice', 'Computer science in sport', 'Critical code studies', 'Developer relations', 'Information and computer science', 'Multicover bifiltration', 'Network Protocol Virtualization', 'Nuclear computation', 'Outline of computer science', 'Prefetching', 'Programming language design and implementation', 'Psychoinformatics', 'Symbolic execution', 'Technology transfer in computer science', 'Transition (computer science)', 'Virtual environment', 'Visible Embryo Project', 'Category:Computer science by country', 'Category:Subfields of computer science', 'Category:Computer scientists', 'Category:History of computer science', 'Category:Computer science awards', 'Category:Computer algebra', 'Category:Computer hardware', 'Category:Computer networking', 'Category:Computer science conferences', 'Category:Computer science education', 'Category:Embedded systems', 'Category:Computer engineering', 'Category:Information theory', 'Category:Computer languages', 'Category:Computer science literature', 'Category:Network science', 'Category:Computer science organizations', 'Category:Philosophy of computer science', 'Category:Problems in computer science', 'Category:Programming Concepts', 'Category:Software', 'Category:String (computer science)', 'Category:Computer science suffixes', 'Category:Computer systems', 'Category:Computer science stubs', 'Portal:Geography', 'Geography', 'Outline of geography', 'Wikipedia:Contents/Geography and places', 'Charmant Som', 'Counter-mapping', 'Distance decay', 'Easting and northing', 'Economic restructuring', 'Edgelands', 'Extreme environment', 'Fundamental plane (spherical coordinates)', 'Geo-literacy', 'Geo-replication', 'Geoarchaeology', 'Geocriticism', 'Geographic contiguity', 'Geographic targeting', 'Geographical cluster', 'Geographical feature', 'Geography of aging', 'Geomorphosite', 'Geopark', 'Glacial refugium', 'Governmentality', 'Hemispheres of Earth', 'Hermit kingdom', 'Hjulström curve', 'International date line in Judaism', 'Land cover', 'Land systems', 'Landlocked developing countries', 'Mainland', 'Map', 'Medical geography', 'Mediterranean outflow', 'Mountain pass', 'Mountain research', 'Natural landscape', 'Pan-region', 'Place identity', 'Population density', 'Primary care service area', 'Rank–size distribution', 'Region', 'Riverscape', 'Small Island Developing States', 'Solar equator', 'Spatial citizenship', 'Spatial justice', 'Spatial mismatch', 'Surroundings', 'Topography', 'Category:Geography by location', 'Category:Geography by period', 'Category:Geographers', 'Category:Geography-related lists', 'Category:Geographical areas', 'Category:Geography awards', 'Category:Geographic classifications', 'Category:Geography competitions', 'Category:Geography conferences', 'Category:Deserts', 'Category:Geography education', 'Category:Exploration', 'Category:History of geography', 'Category:Land systems', 'Category:Landscape', 'Category:Laws of geography', 'Category:Geographic literature', 'Category:Maps', 'Category:Meridians (geography)', 'Category:Geography organizations', 'Category:Phantom geographical features', 'Category:Places', 'Category:Geographic position', 'Category:Subfields of geography', 'Category:Geographical superlatives', 'Category:Geography terminology', 'Category:Time zones', 'Category:Types of geographical division', 'Category:Works about geography', 'Category:World maps', 'Category:Geographical zones', 'Category:Geographic images', 'Category:Geography stubs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "filtered_text = [preprocess_text(doc) for doc in article_df['text']]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(filtered_text)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynWgFqBiwM9D",
        "outputId": "e0282759-f02f-4358-da25-e5a17aa8ef4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['11' 'about' 'activism' 'aerospike' 'aging' 'agnostic' 'algebra'\n",
            " 'alternative' 'and' 'area' 'areas' 'aspects' 'asset' 'assisted'\n",
            " 'associations' 'attacks' 'awards' 'bajrangarh' 'bifiltration' 'biography'\n",
            " 'boolean' 'bug' 'by' 'care' 'category' 'catering' 'cause' 'century'\n",
            " 'charenton' 'charmant' 'chemical' 'chemicals' 'choice' 'chronology'\n",
            " 'citizenship' 'city' 'civilization' 'classifications' 'clinical'\n",
            " 'cluster' 'code' 'competitions' 'compressed' 'computation'\n",
            " 'computational' 'computer' 'concepts' 'conferences' 'conspiracy'\n",
            " 'construction' 'consulting' 'contact' 'contents' 'contiguity'\n",
            " 'controversies' 'coordinates' 'counter' 'countries' 'country' 'cover'\n",
            " 'critical' 'culture' 'curve' 'data' 'date' 'day' 'death' 'decade' 'decay'\n",
            " 'deities' 'density' 'dependent' 'deserts' 'design' 'detector' 'developer'\n",
            " 'developing' 'development' 'diplomacy' 'disciplines' 'distance'\n",
            " 'distribution' 'division' 'dynasty' 'earth' 'easting' 'economic'\n",
            " 'economy' 'edgelands' 'education' 'embedded' 'embryo' 'engine' 'engineer'\n",
            " 'engineering' 'engineers' 'environment' 'environmental' 'equator'\n",
            " 'equipment' 'etching' 'ethnic' 'european' 'events' 'execution'\n",
            " 'exploration' 'extreme' 'ezio' 'families' 'feature' 'features' 'fields'\n",
            " 'figure' 'for' 'fort' 'fundamental' 'generalized' 'geo' 'geoarchaeology'\n",
            " 'geocriticism' 'geographers' 'geographic' 'geographical' 'geography'\n",
            " 'geomorphosite' 'geopark' 'glacial' 'glossary' 'governmentality' 'group'\n",
            " 'hardware' 'health' 'hemispheres' 'hermit' 'hijackers' 'historic'\n",
            " 'historical' 'historicism' 'historicity' 'historiography' 'history'\n",
            " 'hjulström' 'hockey' 'house' 'humanities' 'ice' 'identity' 'images'\n",
            " 'implementation' 'in' 'industrial' 'information' 'inhibition' 'insurance'\n",
            " 'integrity' 'international' 'intersex' 'island' 'judaism' 'justice'\n",
            " 'kingdom' 'land' 'landlocked' 'landscape' 'language' 'languages' 'laws'\n",
            " 'legacies' 'line' 'list' 'lists' 'literacy' 'literature' 'local'\n",
            " 'location' 'magic' 'mainland' 'management' 'mancos' 'manufacturing' 'map'\n",
            " 'mapping' 'maps' 'marine' 'medical' 'medicine' 'mediterranean' 'medium'\n",
            " 'memory' 'meridians' 'metal' 'method' 'metro' 'microelectronics'\n",
            " 'mismatch' 'monitoring' 'mountain' 'multicover' 'nanowire' 'national'\n",
            " 'natural' 'nepal' 'network' 'networking' 'northing' 'nuclear' 'objects'\n",
            " 'occupations' 'of' 'oilfield' 'opera' 'oral' 'organizations' 'origins'\n",
            " 'outflow' 'outline' 'outlines' 'pan' 'pass' 'pdf' 'people' 'period'\n",
            " 'phantom' 'phenomena' 'philosophy' 'photon' 'place' 'places' 'plane'\n",
            " 'population' 'portal' 'position' 'practice' 'prefetching' 'preservation'\n",
            " 'prevention' 'primary' 'problems' 'process' 'profile' 'programming'\n",
            " 'project' 'projects' 'protocol' 'pseudohistory' 'psychoinformatics'\n",
            " 'qing' 'range' 'rank' 'redemptive' 'refugium' 'region' 'related'\n",
            " 'relations' 'religion' 'renewal' 'replication' 'research' 'restructuring'\n",
            " 'riverscape' 'scale' 'school' 'science' 'scientists' 'september'\n",
            " 'service' 'significance' 'single' 'size' 'slr' 'small' 'social' 'society'\n",
            " 'software' 'solar' 'som' 'spatial' 'spherical' 'sport' 'stanford'\n",
            " 'states' 'stepped' 'string' 'stubs' 'studies' 'subdivision' 'subfields'\n",
            " 'suffixes' 'superconducting' 'superlatives' 'surroundings' 'sustainable'\n",
            " 'symbolic' 'symbols' 'systems' 'targeting' 'technology' 'terminology'\n",
            " 'terms' 'territory' 'texts' 'the' 'theories' 'theory' 'third' 'time'\n",
            " 'todini' 'topography' 'transfer' 'transition' 'translation' 'tree'\n",
            " 'types' 'unesco' 'university' 'unsolved' 'variations' 'veterinary'\n",
            " 'viaduct' 'violence' 'virtual' 'virtualization' 'visible' 'volunteerism'\n",
            " 'western' 'wikipedia' 'women' 'works' 'world' 'year' 'zones']\n",
            "BoW Matrix:\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-55c5c8e340f6>:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, 'html.parser')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "labels = article_df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow_matrix, labels, test_size = 0.2, random_state = 37)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "nb_predict = nb_classifier.predict(X_test)\n",
        "\n",
        "lr_classifier = LogisticRegression()\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "lr_predict = lr_classifier.predict(X_test)\n",
        "\n",
        "print('Naive Bayes Accuracy: ', accuracy_score(y_test, nb_predict))\n",
        "print('Naive Bayes Classification:\\n', classification_report(y_test, nb_predict))\n",
        "\n",
        "print('\\nLogistic Regression Accuracy:', accuracy_score(y_test, lr_predict))\n",
        "print('Logistion Regression Classification:\\n', classification_report(y_test, lr_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbriGfIJ18CI",
        "outputId": "780d469e-a87f-4451-a0ef-bd46d2a03b08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy:  0.9384615384615385\n",
            "Naive Bayes Classification:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.92      0.96        51\n",
            "           1       0.78      1.00      0.88        14\n",
            "\n",
            "    accuracy                           0.94        65\n",
            "   macro avg       0.89      0.96      0.92        65\n",
            "weighted avg       0.95      0.94      0.94        65\n",
            "\n",
            "\n",
            "Logistic Regression Accuracy: 0.9846153846153847\n",
            "Logistion Regression Classification:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        51\n",
            "           1       1.00      0.93      0.96        14\n",
            "\n",
            "    accuracy                           0.98        65\n",
            "   macro avg       0.99      0.96      0.98        65\n",
            "weighted avg       0.98      0.98      0.98        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual Labels:\", y_test)\n",
        "print(bow_matrix)\n",
        "print(\"Naive Bayes Predictions:\", nb_predict)\n",
        "print(\"Logistic Regression Predictions:\", lr_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcpyRo7mPQZt",
        "outputId": "64bf9f3f-1faf-4192-d9e8-802a56b5cd93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Labels: 185    0\n",
            "286    0\n",
            "229    0\n",
            "131    0\n",
            "218    0\n",
            "      ..\n",
            "198    0\n",
            "32     1\n",
            "130    0\n",
            "259    0\n",
            "275    0\n",
            "Name: label, Length: 65, dtype: int64\n",
            "  (0, 185)\t1\n",
            "  (1, 185)\t1\n",
            "  (1, 215)\t1\n",
            "  (1, 208)\t1\n",
            "  (2, 185)\t1\n",
            "  (2, 230)\t1\n",
            "  (3, 185)\t1\n",
            "  (3, 24)\t1\n",
            "  (3, 22)\t1\n",
            "  (3, 35)\t1\n",
            "  (4, 185)\t1\n",
            "  (4, 24)\t1\n",
            "  (4, 22)\t1\n",
            "  (4, 58)\t1\n",
            "  (5, 185)\t1\n",
            "  (5, 24)\t1\n",
            "  (5, 22)\t1\n",
            "  (5, 58)\t1\n",
            "  (5, 285)\t1\n",
            "  (6, 185)\t1\n",
            "  (6, 24)\t1\n",
            "  (6, 22)\t1\n",
            "  (6, 71)\t1\n",
            "  (6, 299)\t1\n",
            "  (7, 185)\t1\n",
            "  :\t:\n",
            "  (315, 123)\t1\n",
            "  (316, 24)\t1\n",
            "  (316, 305)\t1\n",
            "  (316, 330)\t1\n",
            "  (317, 208)\t1\n",
            "  (317, 24)\t1\n",
            "  (317, 122)\t1\n",
            "  (317, 312)\t1\n",
            "  (317, 82)\t1\n",
            "  (318, 24)\t1\n",
            "  (318, 327)\t1\n",
            "  (318, 1)\t1\n",
            "  (318, 123)\t1\n",
            "  (319, 24)\t1\n",
            "  (319, 328)\t1\n",
            "  (319, 182)\t1\n",
            "  (320, 24)\t1\n",
            "  (320, 122)\t1\n",
            "  (320, 330)\t1\n",
            "  (321, 24)\t1\n",
            "  (321, 147)\t1\n",
            "  (321, 121)\t1\n",
            "  (322, 24)\t1\n",
            "  (322, 283)\t1\n",
            "  (322, 123)\t1\n",
            "Naive Bayes Predictions: [0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
            "Logistic Regression Predictions: [0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zuba1rkhan/NLP.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsAVl2Wq68hI",
        "outputId": "8de755c9-c376-403e-bc5a-bf6cf2bc5669"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4_y1Fdb7Y1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}